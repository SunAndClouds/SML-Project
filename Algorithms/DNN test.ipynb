{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6abc81a9-ae86-443f-9e1c-02e5ee0ea40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna -q\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 7)\n",
    "plt.rcParams[\"figure.dpi\"] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f7d4f1-6759-4b65-98dd-564979f8af42",
   "metadata": {},
   "source": [
    "# Training the final model with all training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3586db82-e673-4b5e-b0ce-2099c369b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'training_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data['increase_stock_binary'] = data['increase_stock'].map(lambda x: 0 if x == 'low_bike_demand' else 1)\n",
    "data = data.drop(['snow', 'increase_stock'], axis=1)\n",
    "# pick out the labels\n",
    "y = data['increase_stock_binary'].to_numpy().astype(np.float32)\n",
    "y = torch.tensor(y).unsqueeze(-1)\n",
    "data = data.drop(['increase_stock_binary'], axis=1)\n",
    "\n",
    "\n",
    "# Separating the numerical and categorical data to handle them separately\n",
    "onehot = True\n",
    "if onehot:\n",
    "    cat_data = pd.DataFrame({key:data[key] for key in data.keys() if data[key].dtype == int})\n",
    "    num_data = pd.DataFrame({key:data[key] for key in data.keys() if data[key].dtype == float})\n",
    "    \n",
    "    # performing onehot encoding on the categorical data\n",
    "    cat_data = torch.from_numpy(cat_data.to_numpy())\n",
    "    cat_onehot = torch.cat([F.one_hot(x, num_classes=24) for x in cat_data]).view(cat_data.shape[0], -1)\n",
    "    \n",
    "    # constructing the complete input dataset\n",
    "    data = np.concatenate([num_data.to_numpy(), cat_onehot.numpy()], axis=-1).astype(np.float32)\n",
    "\n",
    "# scaling\n",
    "X = MinMaxScaler().fit_transform(data) \n",
    "X = torch.Tensor(X)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, shuffle=True, random_state=0)\n",
    "X_train = X\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "111ea420-e016-4b1d-a96f-8605480ea0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, hidden_size, input_size, layers=2, seed=0, submodule=False):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.submodule = submodule\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.batch_norm_input = nn.BatchNorm1d(hidden_size)  # BatchNorm for input layer\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        self.batch_norm_layers = nn.ModuleList()  # BatchNorm for other layers\n",
    "\n",
    "        for _ in range(layers):\n",
    "            self.linear_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.batch_norm_layers.append(nn.BatchNorm1d(hidden_size))\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.batch_norm_input(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        for layer, batch_norm in zip(self.linear_layers, self.batch_norm_layers):\n",
    "            residual = x\n",
    "            x = layer(x)\n",
    "            x = batch_norm(x)  \n",
    "            x = self.activation(x + residual)\n",
    "        \n",
    "        if not self.submodule:\n",
    "            x = self.output_layer(x)\n",
    "            x = torch.sigmoid(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3505be88-6ad0-4c02-9023-0dfe36927be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.85it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.92it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.16it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 53.81it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.40it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.74it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 53.41it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.77it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.86it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 53.22it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.48it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.93it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.41it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.48it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.19it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.58it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.34it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.38it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.02it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.58it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.27it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.08it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.21it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.26it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.00it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.45it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.93it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 53.59it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.88it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.52it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 53.97it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.27it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.17it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.59it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.66it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 52.82it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.32it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 51.27it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.36it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.01it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.61it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.51it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.05it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.06it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.09it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 51.68it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.25it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.38it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.38it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.24it/s]\n"
     ]
    }
   ],
   "source": [
    "epochs = 100 \n",
    "loss_fn = nn.BCELoss()\n",
    "n_ensembles = 10 # checking the performance for different weight initialization\n",
    "kf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "optimal_models = []\n",
    "\n",
    "for n in range(n_ensembles):\n",
    "    for cv, (train_index, test_index) in enumerate(kf.split(X_train, y_train)):\n",
    "        model = DNN(200, X.shape[-1], 2, seed=n)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1)\n",
    "        \n",
    "        train_loss_history = []\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            optimizer.zero_grad()\n",
    "            model.train()\n",
    "            y_pred = model(X_train[train_index])\n",
    "            loss = loss_fn(y_pred, y_train[train_index])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_history.append(loss.item())\n",
    "            \n",
    "            if loss.item() <= min(train_loss_history):\n",
    "                optimal_weights = model.state_dict()\n",
    "                \n",
    "        optimal_models.append(optimal_weights)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9753173b-d97d-4a47-8c43-d8a7d5630050",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "be00e111-5ee4-4b8c-a5fc-8c111c44f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'test_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data = data.drop(['snow'], axis=1)\n",
    "\n",
    "\n",
    "# Separating the numerical and categorical data to handle them separately\n",
    "onehot = True\n",
    "if onehot:\n",
    "    cat_data = pd.DataFrame({key:data[key] for key in data.keys() if data[key].dtype == int})\n",
    "    num_data = pd.DataFrame({key:data[key] for key in data.keys() if data[key].dtype == float})\n",
    "    \n",
    "    # performing onehot encoding on the categorical data\n",
    "    cat_data = torch.from_numpy(cat_data.to_numpy())\n",
    "    cat_onehot = torch.cat([F.one_hot(x, num_classes=24) for x in cat_data]).view(cat_data.shape[0], -1)\n",
    "    \n",
    "    # constructing the complete input dataset\n",
    "    data = np.concatenate([num_data.to_numpy(), cat_onehot.numpy()], axis=-1).astype(np.float32)\n",
    "\n",
    "# scaling\n",
    "X = MinMaxScaler().fit_transform(data) \n",
    "X_test = torch.Tensor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5e19fc9-d316-41c4-bf57-e36236f42872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepNeuralNetworkEnsemble(optimal_models, test_data):\n",
    "    DNN_ensemble = []\n",
    "    for weights in optimal_models:\n",
    "        model = DNN(200, X.shape[-1], 2)\n",
    "        model.load_state_dict(weights)\n",
    "        y_pred = model(test_data)\n",
    "        DNN_ensemble.append(y_pred.detach().numpy())\n",
    "    return torch.Tensor(DNN_ensemble).mean(0).round()\n",
    "    \n",
    "prediction = DeepNeuralNetworkEnsemble(optimal_models, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b50e56b2-ac0c-4f85-a807-6801f791bb3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 1D or 2D array, got 0D array instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-905528663e5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DNN test prediction.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1395\u001b[0m         \u001b[0;31m# Handle 1-dimensional arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1397\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1398\u001b[0m                 \"Expected 1D or 2D array, got %dD array instead\" % X.ndim)\n\u001b[1;32m   1399\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 1D or 2D array, got 0D array instead"
     ]
    }
   ],
   "source": [
    "np.savetxt(\"DNN test prediction.csv\", prediction.detach().numpy().flatten().astype(int)), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b421c4ae-eff0-48d7-accc-3e09ab3bb7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "',0\\n0,0\\n1,1\\n2,1\\n3,0\\n4,0\\n5,0\\n6,1\\n7,0\\n8,0\\n9,0\\n10,0\\n11,1\\n12,0\\n13,0\\n14,0\\n15,1\\n16,0\\n17,0\\n18,0\\n19,0\\n20,0\\n21,0\\n22,0\\n23,1\\n24,0\\n25,0\\n26,0\\n27,0\\n28,1\\n29,0\\n30,0\\n31,0\\n32,0\\n33,0\\n34,0\\n35,0\\n36,0\\n37,0\\n38,1\\n39,0\\n40,0\\n41,1\\n42,0\\n43,0\\n44,1\\n45,0\\n46,0\\n47,1\\n48,1\\n49,0\\n50,0\\n51,1\\n52,0\\n53,0\\n54,0\\n55,0\\n56,1\\n57,0\\n58,0\\n59,1\\n60,0\\n61,0\\n62,0\\n63,0\\n64,1\\n65,0\\n66,0\\n67,0\\n68,0\\n69,0\\n70,0\\n71,0\\n72,1\\n73,0\\n74,0\\n75,0\\n76,0\\n77,0\\n78,0\\n79,0\\n80,0\\n81,0\\n82,0\\n83,1\\n84,0\\n85,0\\n86,0\\n87,0\\n88,0\\n89,1\\n90,0\\n91,0\\n92,0\\n93,0\\n94,0\\n95,0\\n96,0\\n97,0\\n98,0\\n99,0\\n100,0\\n101,0\\n102,1\\n103,0\\n104,1\\n105,1\\n106,1\\n107,0\\n108,0\\n109,0\\n110,1\\n111,0\\n112,0\\n113,0\\n114,0\\n115,1\\n116,0\\n117,0\\n118,0\\n119,0\\n120,0\\n121,0\\n122,0\\n123,1\\n124,0\\n125,0\\n126,0\\n127,0\\n128,1\\n129,0\\n130,1\\n131,0\\n132,0\\n133,0\\n134,1\\n135,0\\n136,0\\n137,0\\n138,0\\n139,0\\n140,0\\n141,0\\n142,0\\n143,0\\n144,0\\n145,0\\n146,1\\n147,0\\n148,0\\n149,0\\n150,0\\n151,0\\n152,0\\n153,0\\n154,0\\n155,1\\n156,1\\n157,0\\n158,1\\n159,1\\n160,1\\n161,0\\n162,0\\n163,0\\n164,0\\n165,0\\n166,0\\n167,1\\n168,0\\n169,0\\n170,0\\n171,0\\n172,1\\n173,0\\n174,0\\n175,0\\n176,0\\n177,0\\n178,1\\n179,0\\n180,0\\n181,0\\n182,0\\n183,0\\n184,0\\n185,0\\n186,0\\n187,1\\n188,1\\n189,0\\n190,0\\n191,0\\n192,1\\n193,0\\n194,0\\n195,0\\n196,0\\n197,0\\n198,0\\n199,0\\n200,0\\n201,0\\n202,0\\n203,0\\n204,0\\n205,0\\n206,0\\n207,1\\n208,0\\n209,0\\n210,0\\n211,0\\n212,1\\n213,0\\n214,0\\n215,0\\n216,1\\n217,0\\n218,0\\n219,0\\n220,0\\n221,0\\n222,0\\n223,1\\n224,0\\n225,1\\n226,0\\n227,0\\n228,1\\n229,0\\n230,0\\n231,1\\n232,0\\n233,0\\n234,0\\n235,0\\n236,0\\n237,0\\n238,1\\n239,0\\n240,0\\n241,0\\n242,0\\n243,0\\n244,0\\n245,1\\n246,0\\n247,1\\n248,0\\n249,0\\n250,0\\n251,0\\n252,0\\n253,1\\n254,0\\n255,0\\n256,0\\n257,0\\n258,0\\n259,0\\n260,1\\n261,0\\n262,1\\n263,0\\n264,0\\n265,1\\n266,0\\n267,1\\n268,0\\n269,0\\n270,0\\n271,0\\n272,0\\n273,0\\n274,0\\n275,0\\n276,0\\n277,0\\n278,0\\n279,0\\n280,0\\n281,0\\n282,0\\n283,0\\n284,0\\n285,0\\n286,0\\n287,0\\n288,0\\n289,0\\n290,1\\n291,1\\n292,0\\n293,0\\n294,0\\n295,1\\n296,0\\n297,0\\n298,1\\n299,0\\n300,0\\n301,0\\n302,0\\n303,1\\n304,0\\n305,0\\n306,0\\n307,0\\n308,0\\n309,0\\n310,0\\n311,0\\n312,0\\n313,1\\n314,0\\n315,0\\n316,0\\n317,0\\n318,1\\n319,0\\n320,0\\n321,0\\n322,0\\n323,1\\n324,0\\n325,0\\n326,0\\n327,0\\n328,0\\n329,0\\n330,0\\n331,0\\n332,0\\n333,0\\n334,0\\n335,0\\n336,0\\n337,0\\n338,0\\n339,0\\n340,0\\n341,0\\n342,0\\n343,0\\n344,0\\n345,0\\n346,0\\n347,1\\n348,1\\n349,0\\n350,0\\n351,0\\n352,0\\n353,0\\n354,0\\n355,0\\n356,0\\n357,0\\n358,0\\n359,0\\n360,1\\n361,1\\n362,0\\n363,1\\n364,0\\n365,0\\n366,0\\n367,0\\n368,0\\n369,0\\n370,0\\n371,0\\n372,0\\n373,0\\n374,1\\n375,0\\n376,0\\n377,0\\n378,0\\n379,0\\n380,0\\n381,0\\n382,0\\n383,0\\n384,0\\n385,0\\n386,0\\n387,0\\n388,0\\n389,0\\n390,0\\n391,0\\n392,0\\n393,0\\n394,1\\n395,0\\n396,0\\n397,0\\n398,0\\n399,0\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
