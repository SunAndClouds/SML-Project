{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6abc81a9-ae86-443f-9e1c-02e5ee0ea40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna -q\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 7)\n",
    "plt.rcParams[\"figure.dpi\"] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f7d4f1-6759-4b65-98dd-564979f8af42",
   "metadata": {},
   "source": [
    "# Training the final model with all training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3586db82-e673-4b5e-b0ce-2099c369b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'training_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data['increase_stock_binary'] = data['increase_stock'].map(lambda x: 0 if x == 'low_bike_demand' else 1)\n",
    "data = data.drop(['snow', 'increase_stock'], axis=1)\n",
    "# pick out the labels\n",
    "y = data['increase_stock_binary'].to_numpy().astype(np.float32)\n",
    "y = torch.tensor(y).unsqueeze(-1)\n",
    "data = data.drop(['increase_stock_binary'], axis=1)\n",
    "\n",
    "\n",
    "# Separating the numerical and categorical data to handle them separately\n",
    "onehot = True\n",
    "if onehot:\n",
    "    cat_data = pd.DataFrame({key:data[key] for key in data.keys() if data[key].dtype == int})\n",
    "    num_data = pd.DataFrame({key:data[key] for key in data.keys() if data[key].dtype == float})\n",
    "    \n",
    "    # performing onehot encoding on the categorical data\n",
    "    cat_data = torch.from_numpy(cat_data.to_numpy())\n",
    "    cat_onehot = torch.cat([F.one_hot(x, num_classes=24) for x in cat_data]).view(cat_data.shape[0], -1)\n",
    "    \n",
    "    # constructing the complete input dataset\n",
    "    data = np.concatenate([num_data.to_numpy(), cat_onehot.numpy()], axis=-1).astype(np.float32)\n",
    "\n",
    "# scaling\n",
    "X = MinMaxScaler().fit_transform(data) \n",
    "X = torch.Tensor(X)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, shuffle=True, random_state=0)\n",
    "X_train = X\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "111ea420-e016-4b1d-a96f-8605480ea0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, hidden_size, input_size, layers=2, seed=0, submodule=False):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.submodule = submodule\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.batch_norm_input = nn.BatchNorm1d(hidden_size)  # BatchNorm for input layer\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        self.batch_norm_layers = nn.ModuleList()  # BatchNorm for other layers\n",
    "\n",
    "        for _ in range(layers):\n",
    "            self.linear_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.batch_norm_layers.append(nn.BatchNorm1d(hidden_size))\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.batch_norm_input(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        for layer, batch_norm in zip(self.linear_layers, self.batch_norm_layers):\n",
    "            residual = x\n",
    "            x = layer(x)\n",
    "            x = batch_norm(x)  \n",
    "            x = self.activation(x + residual)\n",
    "        \n",
    "        if not self.submodule:\n",
    "            x = self.output_layer(x)\n",
    "            x = torch.sigmoid(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3505be88-6ad0-4c02-9023-0dfe36927be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 52.85it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.92it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.16it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 53.81it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.40it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.74it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 53.41it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.77it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.86it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 53.22it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.48it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.93it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.41it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.48it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.19it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.58it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.34it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.38it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.02it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.58it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.27it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.08it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.21it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.26it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 55.00it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.45it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.93it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 53.59it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.88it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.52it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 53.97it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.27it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.17it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.59it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.66it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 52.82it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.32it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 51.27it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.36it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.01it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.61it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.51it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.05it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.06it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.09it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 51.68it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.25it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.38it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.38it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 54.24it/s]\n"
     ]
    }
   ],
   "source": [
    "epochs = 100 \n",
    "loss_fn = nn.BCELoss()\n",
    "n_ensembles = 10 # checking the performance for different weight initialization\n",
    "kf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "optimal_models = []\n",
    "\n",
    "for n in range(n_ensembles):\n",
    "    for cv, (train_index, test_index) in enumerate(kf.split(X_train, y_train)):\n",
    "        model = DNN(200, X.shape[-1], 2, seed=n)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1)\n",
    "        \n",
    "        train_loss_history = []\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            optimizer.zero_grad()\n",
    "            model.train()\n",
    "            y_pred = model(X_train[train_index])\n",
    "            loss = loss_fn(y_pred, y_train[train_index])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_history.append(loss.item())\n",
    "            \n",
    "            if loss.item() <= min(train_loss_history):\n",
    "                optimal_weights = model.state_dict()\n",
    "                \n",
    "        optimal_models.append(optimal_weights)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9753173b-d97d-4a47-8c43-d8a7d5630050",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "be00e111-5ee4-4b8c-a5fc-8c111c44f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'test_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data = data.drop(['snow'], axis=1)\n",
    "\n",
    "\n",
    "# Separating the numerical and categorical data to handle them separately\n",
    "onehot = True\n",
    "if onehot:\n",
    "    cat_data = pd.DataFrame({key:data[key] for key in data.keys() if data[key].dtype == int})\n",
    "    num_data = pd.DataFrame({key:data[key] for key in data.keys() if data[key].dtype == float})\n",
    "    \n",
    "    # performing onehot encoding on the categorical data\n",
    "    cat_data = torch.from_numpy(cat_data.to_numpy())\n",
    "    cat_onehot = torch.cat([F.one_hot(x, num_classes=24) for x in cat_data]).view(cat_data.shape[0], -1)\n",
    "    \n",
    "    # constructing the complete input dataset\n",
    "    data = np.concatenate([num_data.to_numpy(), cat_onehot.numpy()], axis=-1).astype(np.float32)\n",
    "\n",
    "# scaling\n",
    "X = MinMaxScaler().fit_transform(data) \n",
    "X_test = torch.Tensor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5e19fc9-d316-41c4-bf57-e36236f42872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepNeuralNetworkEnsemble(optimal_models, test_data):\n",
    "    DNN_ensemble = []\n",
    "    for weights in optimal_models:\n",
    "        model = DNN(200, X.shape[-1], 2)\n",
    "        model.load_state_dict(weights)\n",
    "        y_pred = model(test_data)\n",
    "        DNN_ensemble.append(y_pred.detach().numpy())\n",
    "    return torch.Tensor(DNN_ensemble).mean(0).round()\n",
    "    \n",
    "prediction = DeepNeuralNetworkEnsemble(optimal_models, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b50e56b2-ac0c-4f85-a807-6801f791bb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.savetxt(\"DNN test prediction .csv\", prediction.detach().numpy().flatten().astype(int), delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
