{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 \t Training Loss: 0.9090\n",
      "Epoch 20/100 \t Training Loss: 0.8169\n",
      "Epoch 30/100 \t Training Loss: 0.7556\n",
      "Epoch 40/100 \t Training Loss: 0.7348\n",
      "Epoch 50/100 \t Training Loss: 0.7163\n",
      "Epoch 60/100 \t Training Loss: 0.7038\n",
      "Epoch 70/100 \t Training Loss: 0.6982\n",
      "Epoch 80/100 \t Training Loss: 0.6941\n",
      "Epoch 90/100 \t Training Loss: 0.6921\n",
      "Epoch 100/100 \t Training Loss: 0.6832\n",
      "(1200, 12) (400, 12)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'training_data.csv'  # Update with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Encode the target variable as binary\n",
    "data['increase_stock_binary'] = data['increase_stock'].apply(lambda x: 1 if x == 'high_bike_demand' else 0)\n",
    "\n",
    "# Drop the original target variable and separate features and the new target variable\n",
    "X = data.drop(columns=['increase_stock', 'increase_stock_binary'])\n",
    "y = data['increase_stock_binary']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Convert the scaled training and test data to PyTorch tensors\n",
    "X_train_tensor = torch.Tensor(X_train)\n",
    "X_test_tensor = torch.Tensor(X_test)\n",
    "\n",
    "# Create DataLoader for both training and test set\n",
    "batch_size = 128\n",
    "train_dataset = TensorDataset(X_train_tensor, X_train_tensor)  # Autoencoder outputs are the same as inputs\n",
    "test_dataset = TensorDataset(X_test_tensor, X_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the Autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_features, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 12)  # compress to 3 features which is the size of our encoded representations\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, n_features),\n",
    "            nn.Sigmoid()  # since the input is scaled between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the network\n",
    "n_features = X_train_tensor.shape[1]\n",
    "autoencoder = Autoencoder(n_features)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    autoencoder.train()\n",
    "    train_loss = 0.0\n",
    "    for data in train_loader:\n",
    "        inputs, _ = data  # No target variable needed for autoencoder\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(inputs)\n",
    "        loss = criterion(outputs, inputs)  # Compare outputs with inputs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*inputs.size(0)\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Print out the loss periodically\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{n_epochs} \\t Training Loss: {train_loss:.4f}')\n",
    "\n",
    "# Encoding the training set to get the compressed representation\n",
    "autoencoder.eval()  # Evaluation mode\n",
    "X_train_encoded = autoencoder.encoder(X_train_tensor).detach().numpy()\n",
    "X_test_encoded = autoencoder.encoder(X_test_tensor).detach().numpy()\n",
    "\n",
    "print(X_train_encoded.shape, X_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'metric': 'euclidean', 'n_neighbors': 14, 'weights': 'distance'}\n",
      "Best Score: 0.8641666666666665\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the parameter grid for KNN\n",
    "param_grid = {\n",
    "    'n_neighbors': range(1,31),  # Different values for n_neighbors\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# Initialize KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=StratifiedKFold(n_splits=5), scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the encoded data\n",
    "grid_search.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print('Best Parameters:', best_params)\n",
    "print('Best Score:', best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
